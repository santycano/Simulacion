{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import category_encoders as ce\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import preprocessing\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import imblearn\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#df = pd.read_csv('/kaggle/input/diabetic-patients-readmission-prediction/diabetic_data.csv')\n",
    "df = pd.read_csv('diabetic_data.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(df.patient_nbr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial depuration and label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_outliers(df, out_cols, T=1.5, verbose=True):\n",
    "    # Copy of df\n",
    "    new_df = df.copy()\n",
    "    init_shape = new_df.shape\n",
    "    # For each column\n",
    "    for c in out_cols:\n",
    "        q1 = new_df[c].quantile(.25)\n",
    "        q3 = new_df[c].quantile(.75)\n",
    "        col_iqr = q3 - q1\n",
    "        col_max = q3 + T * col_iqr\n",
    "        col_min = q1 - T * col_iqr\n",
    "        # Filter data without outliers and ignoring nan\n",
    "        filtered_df = new_df[(new_df[c] <= col_max) & (new_df[c] >= col_min)]\n",
    "        if verbose:\n",
    "            n_out = new_df.shape[0] - filtered_df.shape[0] \n",
    "            print(f\" Columns {c} had {n_out} outliers removed\")\n",
    "        new_df = filtered_df\n",
    "            \n",
    "    if verbose:\n",
    "        # Print shrink percentage\n",
    "        lines_red = df.shape[0] - new_df.shape[0]\n",
    "        print(f\"Data reduced by {lines_red} lines, or {lines_red/df.shape[0]*100:.2f} %\")\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Data Cleaning\n",
    "df = df.drop(columns=['weight', 'payer_code', 'medical_specialty','admission_type_id','discharge_disposition_id','admission_source_id'])\n",
    "df = df.dropna()\n",
    "df = df[df.race != '?']\n",
    "df = df[df.diag_1 != '?']\n",
    "df = df[df.diag_2 != '?']\n",
    "df = df[df.diag_3 != '?']\n",
    "df = df[df.gender != 'Unknown/Invalid']\n",
    "df = df[df.number_outpatient < 17]\n",
    "df = df[df.number_emergency < 14]\n",
    "df = df[df.number_inpatient < 14]\n",
    "df = df[df.number_diagnoses < 10]\n",
    "df = df[df.number_diagnoses < 10]\n",
    "df.number_outpatient = df.number_outpatient.mask(df.number_outpatient > 7, 8)\n",
    "df.number_emergency = df.number_emergency.mask(df.number_emergency > 5, 6)\n",
    "df.number_inpatient = df.number_inpatient.mask(df.number_inpatient > 9, 10)\n",
    "df = remove_outliers(df, ['num_lab_procedures','num_medications'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Labeling\n",
    "diseases = ['metformin', 'repaglinide', 'nateglinide',\n",
    "'chlorpropamide', 'glimepiride', 'acetohexamide', 'glipizide',\n",
    "'glyburide', 'tolbutamide', 'pioglitazone', 'rosiglitazone', 'acarbose',\n",
    "'miglitol', 'troglitazone', 'tolazamide', 'examide', 'citoglipton',\n",
    "'insulin', 'glyburide-metformin', 'glipizide-metformin',\n",
    "'glimepiride-pioglitazone', 'metformin-rosiglitazone',\n",
    "'metformin-pioglitazone', 'change']\n",
    "\n",
    "df = df.replace({'readmitted' : { 'NO' : 0, '<30' : 2, '>30' : 1}}) #output feacture\n",
    "df = df.replace({'diabetesMed' : { 'No' : 0, 'Yes' : 1}})\n",
    "df = df.replace({'max_glu_serum' : { 'None' : 0, 'Norm' : 1, '>200' : 2, '>300' : 3}})\n",
    "df = df.replace({'A1Cresult' : { 'None' : 0, 'Norm' : 1, '>7' : 2, '>8' : 3}})\n",
    "df = df.replace({'gender' : { 'Female' : 0, 'Male' : 1}})\n",
    "df = df.replace({'race' : { 'AfricanAmerican' : 0, 'Asian' : 1, 'Caucasian' : 2, 'Hispanic' : 3, 'Other' : 4}})\n",
    "df = df.replace({'age' : { 'Female' : 0, 'Male' : 1}})\n",
    "df = df.replace({'age' : { '[0-10)' : 0, '[10-20)' : 1, '[20-30)' : 2, '[30-40)' : 3, '[40-50)' : 4,\n",
    "                           '[50-60)' : 5, '[60-70)' : 6, '[70-80)' : 7, '[80-90)' : 8, '[90-100)' : 9}})\n",
    "\n",
    "opts = {'No': 0, 'Down': 1, 'Steady': 1, 'Up': 1}\n",
    "opts1 = {'No': 0, 'Steady': 1}\n",
    "opts2 = {'No': 0}\n",
    "opts3 = {'No': 0, 'Steady': 1, 'Up': 1}\n",
    "opts4 = {'No': 0, 'Ch' : 1}\n",
    "\n",
    "encoding = []\n",
    "\n",
    "for i in diseases:\n",
    "    if set(df[i]) == opts.keys(): labeling = {i : opts} \n",
    "    elif set(df[i]) == opts1.keys(): labeling = {i : opts1} \n",
    "    elif set(df[i]) == opts2.keys(): labeling = {i : opts2}\n",
    "    elif set(df[i]) == opts3.keys(): labeling = {i : opts3} \n",
    "    elif set(df[i]) == opts4.keys(): labeling = {i : opts4} \n",
    "    df = df.replace(labeling)\n",
    "    encoding.append([labeling])\n",
    "    \n",
    "df['minor_diabetes_med'] = df.chlorpropamide + df.tolbutamide + df.miglitol+ df.troglitazone+ df.tolazamide+ df['glipizide-metformin']+ df['glimepiride-pioglitazone']+ df['metformin-rosiglitazone']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def diagTreatmeant(dataframe):\n",
    "    exclude = {'428':19 ,'414':20 ,'786':21 ,'486':22 ,'410':23 ,'427':24 ,'491':25, '715':26 ,'434':26 ,'780':27 ,'682':28 ,'996':28 ,'276':29 ,'250.8':30}\n",
    "    newData = []\n",
    "    for i in dataframe:\n",
    "        if i.find('V') == -1 and i.find('E') == -1:\n",
    "            tmp = float(i)\n",
    "            if not i in exclude.keys():\n",
    "                if tmp >= 1 and tmp <= 139:\n",
    "                    newData.append(1)\n",
    "                elif tmp >= 140 and tmp <= 239:\n",
    "                    newData.append(2)\n",
    "                elif tmp >= 240 and tmp <= 279:\n",
    "                    newData.append(3)\n",
    "                elif tmp >= 280 and tmp <= 289:\n",
    "                    newData.append(4)\n",
    "                elif tmp >= 290 and tmp <= 319:\n",
    "                    newData.append(5)\n",
    "                elif tmp >= 320 and tmp <= 389:\n",
    "                    newData.append(6)\n",
    "                elif tmp >= 390 and tmp <= 459:\n",
    "                    newData.append(7)\n",
    "                elif tmp >= 460 and tmp <= 519:\n",
    "                    newData.append(8)\n",
    "                elif tmp >= 520 and tmp <= 579:\n",
    "                    newData.append(9)\n",
    "                elif tmp >= 580 and tmp <= 629:\n",
    "                    newData.append(10)\n",
    "                elif tmp >= 630 and tmp <= 679:\n",
    "                    newData.append(11)\n",
    "                elif tmp >= 680 and tmp <= 709:\n",
    "                    newData.append(12)\n",
    "                elif tmp >= 710 and tmp <= 739:\n",
    "                    newData.append(13)\n",
    "                elif tmp >= 740 and tmp <= 759:\n",
    "                    newData.append(14)\n",
    "                elif tmp >= 760 and tmp <= 779:\n",
    "                    newData.append(15)\n",
    "                elif tmp >= 780 and tmp <= 799:\n",
    "                    newData.append(16)\n",
    "                else:\n",
    "                    newData.append(17)\n",
    "            else:\n",
    "                newData.append(exclude[i])\n",
    "        else:\n",
    "            if i.find('V') != -1:\n",
    "                newData.append(18)\n",
    "            else:\n",
    "                newData.append(31)\n",
    "    return newData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split output feacture\n",
    "data = df.copy()\n",
    "data = data.drop_duplicates(subset=['patient_nbr'])\n",
    "data = data.drop(columns=['encounter_id', 'patient_nbr'])\n",
    "y = pd.DataFrame(data.readmitted.copy())\n",
    "data = data.drop(columns=['readmitted'])\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Diagnostic gouping accord to ICD9\n",
    "data.diag_1 = diagTreatmeant(data.diag_1)\n",
    "data.diag_2 = diagTreatmeant(data.diag_2)\n",
    "data.diag_3 = diagTreatmeant(data.diag_3)\n",
    "data = data.drop(columns=['acetohexamide', 'examide', 'citoglipton', 'metformin-rosiglitazone', 'metformin-pioglitazone', 'chlorpropamide','tolbutamide','miglitol','troglitazone','tolazamide','glipizide-metformin','glimepiride-pioglitazone'])\n",
    "ordinal_features = ['diag_1','diag_2', 'diag_3', 'race', 'age']\n",
    "ce_ord = ce.TargetEncoder(cols = ordinal_features)\n",
    "data = ce_ord.fit_transform(data, y)\n",
    "ce_by = ce.BinaryEncoder(cols = diseases)\n",
    "data = ce_ord.fit_transform(data, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Numeric data scale\n",
    "numeric_feactures = ['time_in_hospital', 'num_lab_procedures', 'num_procedures', 'num_medications', 'number_outpatient', 'number_emergency', 'number_inpatient', 'number_diagnoses']\n",
    "scaler = MinMaxScaler()\n",
    "data[numeric_feactures] = scaler.fit_transform(data[numeric_feactures])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feacture correlation\n",
    "plt.figure(figsize=(30, 20))\n",
    "sns.heatmap(data.corr(method = 'kendall'), annot = data.corr(), square = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['readmitted'] = y "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No readmitted, 30< readmitted model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "process_data = data.copy()\n",
    "process_data = process_data[process_data.readmitted != 1]\n",
    "process_data = process_data.replace({'readmitted' : { 2 : 1}}) #output feacture\n",
    "y = pd.DataFrame(process_data.readmitted.copy())\n",
    "process_data = process_data.drop(columns=['readmitted'])\n",
    "#Label 0: No readmitted\n",
    "#Label 1: 30< readmitted\n",
    "y.value_counts().plot.bar(xlabel=['No readmitted','30< readmitted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Train test scolumnst\n",
    "def train_test_split_generator(data, y, smote):   \n",
    "    x_train, x_test, y_train, y_test = train_test_split(data, y ,test_size = 0.2, random_state=42, stratify = y)\n",
    "    \n",
    "    if (smote):\n",
    "        sm = BorderlineSMOTE(random_state=42,k_neighbors=3)\n",
    "        x_train, y_train = sm.fit_resample(x_train, y_train)\n",
    "    \n",
    "    print(y_train.value_counts().sort_index())\n",
    "    print(y_test.value_counts().sort_index())\n",
    "    \n",
    "    x_train = x_train.astype('float64')\n",
    "    y_train = y_train.astype('int')\n",
    "\n",
    "    x_train=x_train.values\n",
    "    y_train=y_train.values.reshape((-1))\n",
    "\n",
    "\n",
    "    x_test = x_test.astype('float64')\n",
    "    y_test = y_test.astype('int')\n",
    "\n",
    "    x_test=x_test.values\n",
    "    y_test=y_test.values.reshape((-1))\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test\n",
    "    \n",
    "x_train, x_test, y_train, y_test = train_test_split_generator(process_data, y, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Class weight calculation\n",
    "class_weights = dict(zip(np.unique(y_train), class_weight.compute_class_weight('balanced', np.unique(y_train),y_train))) \n",
    "sample_weights = [class_weights[y] for y in y_train]\n",
    "train_sample_weight = sample_weights\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#RandomForestClassifier\n",
    "param_grid = {\n",
    "    'max_depth': [6, 10, 17, 40],\n",
    "    'n_estimators': [50, 100, 200, 300, 1000]\n",
    "}\n",
    "clf = RandomForestClassifier(class_weight=class_weights)\n",
    "param_grid = GridSearchCV(estimator = clf, scoring = 'roc_auc_ovr', param_grid = param_grid, cv = StratifiedKFold(n_splits=5), verbose=4, n_jobs = -1)\n",
    "param_grid.fit(x_train, y_train)\n",
    "param_grid.best_params_\n",
    "#6-300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# GradientBoostingClassifier\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 300, 1000],\n",
    "    'learning_rate' : [0.01, 0.1, 0.001]\n",
    "}\n",
    "\n",
    "clf = GradientBoostingClassifier()\n",
    "grid_search = GridSearchCV(estimator = clf, scoring = 'roc_auc_ovo', param_grid = param_grid, cv = StratifiedKFold(n_splits=5), verbose=10, n_jobs = -1)\n",
    "grid_search.fit(x_train, y_train)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "# GradientBoostingClassifier\n",
    "param_grid = {\n",
    "    'reg_param': [0.0001, 0.001,0.01, 0.1], \n",
    "    'store_covariance': [True, False],\n",
    "    'tol': [0.0001, 0.001,0.01, 0.1], \n",
    "}\n",
    "\n",
    "clf = QuadraticDiscriminantAnalysis()\n",
    "grid_search = GridSearchCV(estimator = clf, scoring = 'roc_auc_ovo', param_grid = param_grid, cv = StratifiedKFold(n_splits=5), verbose=10, n_jobs = -1)\n",
    "grid_search.fit(x_train, y_train)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# GradientBoostingClassifier\n",
    "param_grid = {\n",
    "    'C': [0.1,1, 10, 100], \n",
    "    'gamma': [1,0.1,0.01,0.001],\n",
    "    'kernel': ['rbf']\n",
    "}\n",
    "\n",
    "clf = SVC()\n",
    "grid_search = GridSearchCV(estimator = clf, scoring = 'roc_auc_ovo', param_grid = param_grid, cv = StratifiedKFold(n_splits=5), verbose=10, n_jobs = -1)\n",
    "grid_search.fit(x_train, y_train)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.optimizers import adam_v2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n",
    "                              patience=8, min_lr=1e-6,verbose = 1)\n",
    "custom_early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', mode='min', verbose=1, patience=20\n",
    ")\n",
    "\n",
    "METRICS = [ \n",
    "      keras.metrics.AUC(name='auc'),\n",
    "]\n",
    "\n",
    "d1 = 0.15\n",
    "\n",
    "\n",
    "#model definition\n",
    "def create_model(learn_rate):\n",
    "    model = keras.Sequential([\n",
    "    keras.layers.Dense(100, activation=tf.nn.selu, kernel_initializer = initializers.he_normal,bias_initializer=initializers.he_normal, input_shape=(x_train.shape[1],)),\n",
    "        keras.layers.Dropout(d1),\n",
    "        keras.layers.BatchNormalization(),\n",
    "    \n",
    "    keras.layers.Dense(90, activation=tf.nn.selu, kernel_constraint=max_norm(max_n), kernel_initializer = initializers.he_normal),\n",
    "        keras.layers.Dropout(d1),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        \n",
    "    keras.layers.Dense(80, activation=tf.nn.selu, kernel_constraint=max_norm(max_n), kernel_initializer = initializers.he_normal),\n",
    "        keras.layers.Dropout(d1),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        \n",
    "    keras.layers.Dense(70, activation=tf.nn.selu, kernel_constraint=max_norm(max_n), kernel_initializer = initializers.he_normal),\n",
    "        keras.layers.Dropout(d1),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        \n",
    "    keras.layers.Dense(60, activation=tf.nn.selu, kernel_constraint=max_norm(max_n), kernel_initializer = initializers.he_normal),\n",
    "        keras.layers.Dropout(d1),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        \n",
    "    keras.layers.Dense(50, activation=tf.nn.selu, kernel_constraint=max_norm(max_n), kernel_initializer = initializers.he_normal),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        \n",
    "    keras.layers.Dense(40, activation=tf.nn.selu, kernel_constraint=max_norm(max_n), kernel_initializer = initializers.he_normal),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        \n",
    "    keras.layers.Dense(30, activation=tf.nn.selu, kernel_constraint=max_norm(max_n), kernel_initializer = initializers.he_normal),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        \n",
    "    keras.layers.Dense(20, activation=tf.nn.selu, kernel_constraint=max_norm(max_n), kernel_initializer = initializers.he_normal),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        \n",
    "    keras.layers.Dense(10, activation=tf.nn.selu, kernel_constraint=max_norm(max_n), kernel_initializer = initializers.he_normal),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        \n",
    " keras.layers.Dense(1, activation=  'sigmoid')\n",
    " ])\n",
    "    optimizer = adam_v2.Adam(lr=learn_rate)\n",
    "    model.compile(loss='binary_crossentropy', metrics=[METRICS], optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Merge inputs and targets\n",
    "inputs = np.concatenate((x_train, x_test), axis=0)\n",
    "targets = np.concatenate((y_train, y_test), axis=0)\n",
    "\n",
    "max_n = 4.\n",
    "num_folds = 5\n",
    "\n",
    "# Define the K-fold Cross Validator\n",
    "kfold = StratifiedKFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "fold_no = 1\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "confusions_matrix = []\n",
    "for train, test in kfold.split(inputs, targets):\n",
    "      # Compile the model\n",
    "    model = create_model(0.1)\n",
    "    \n",
    "    X_train_res, y_train_res = inputs[train], targets[train]\n",
    "    X_test_res, y_test_res = inputs[test], targets[test]\n",
    "        \n",
    "      # Generate a print\n",
    "    print('-----------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "    # Fit data to model , class_weight=class_weight\n",
    "    history = model.fit(X_train_res, y_train_res, class_weight=class_weights, validation_data=(X_test_res, y_test_res),batch_size=256, epochs=100,callbacks=[custom_early_stopping, reduce_lr])\n",
    "\n",
    "    # Generate generalization metrics\n",
    "    scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
    "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "    acc_per_fold.append(scores[1] * 100)\n",
    "    loss_per_fold.append(scores[0])\n",
    "    \n",
    "    fig, axs = plt.subplots(2)\n",
    "    fig.suptitle('model loss')\n",
    "    fig.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "    axs[0].plot(history.history['loss'])\n",
    "    axs[0].plot(history.history['val_loss'])\n",
    "    axs[1].plot(history.history['auc'])\n",
    "    axs[1].plot(history.history['val_auc'])\n",
    "\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.show()\n",
    "\n",
    "      # Increase fold number\n",
    "    fold_no = fold_no + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf_Q1 = QuadraticDiscriminantAnalysis(reg_param=0.0001, store_covariance=True, tol=0.0001).fit(x_train, y_train)\n",
    "clf_S1 = SVC(C=0.1, gamma=1, kernel='rbf').fit(x_train, y_train, sample_weight=train_sample_weight)\n",
    "clf_G1 = GradientBoostingClassifier(learning_rate=0.01, n_estimators=1000).fit(x_train, y_train)\n",
    "clf_R1 = RandomForestClassifier(max_depth=6, n_estimators=300,class_weight=class_weights).fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict QuadraticDiscriminantAnalysis\n",
    "y_1 = clf_Q1.predict(x_test)\n",
    "score = roc_auc_score(y_test, y_1)\n",
    "#score = roc_auc_score(y_test, clf_1.predict_proba(x_test), multi_class='ovr')\n",
    "print('ROC AUC: %.3f' % score)\n",
    "print(classification_report_imbalanced(y_test, y_1))\n",
    "cm = metrics.confusion_matrix(y_test, y_1, normalize='true')\n",
    "cm_display = metrics.ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict SVC\n",
    "y_1 = clf_S1.predict(x_test)\n",
    "score = roc_auc_score(y_test, y_1)\n",
    "#score = roc_auc_score(y_test, clf_1.predict_proba(x_test), multi_class='ovr')\n",
    "print('ROC AUC: %.3f' % score)\n",
    "print(classification_report_imbalanced(y_test, y_1))\n",
    "cm = metrics.confusion_matrix(y_test, y_1, normalize='true')\n",
    "cm_display = metrics.ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict GradientBoostingClassifier\n",
    "y_1 = clf_G1.predict(x_test)\n",
    "score = roc_auc_score(y_test, y_1)\n",
    "#score = roc_auc_score(y_test, clf_1.predict_proba(x_test), multi_class='ovr')\n",
    "print('ROC AUC: %.3f' % score)\n",
    "print(classification_report_imbalanced(y_test, y_1))\n",
    "cm = metrics.confusion_matrix(y_test, y_1, normalize='true')\n",
    "cm_display = metrics.ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict RandomForestClassifier\n",
    "y_1 = clf_R1.predict(x_test)\n",
    "score = roc_auc_score(y_test, y_1)\n",
    "#score = roc_auc_score(y_test, clf_1.predict_proba(x_test), multi_class='ovr')\n",
    "print('ROC AUC: %.3f' % score)\n",
    "print(classification_report_imbalanced(y_test, y_1))\n",
    "cm = metrics.confusion_matrix(y_test, y_1, normalize='true')\n",
    "cm_display = metrics.ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "y_pred = model.predict(x_test)\n",
    "y_1 = np.round(y_pred)\n",
    "score = roc_auc_score(y_test, y_1)\n",
    "#score = roc_auc_score(y_test, clf_1.predict_proba(x_test), multi_class='ovr')\n",
    "print('ROC AUC: %.3f' % score)\n",
    "print(classification_report_imbalanced(y_test, y_1))\n",
    "cm = metrics.confusion_matrix(y_test, y_1, normalize='true')\n",
    "cm_display = metrics.ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split_generator(process_data, y, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict QuadraticDiscriminantAnalysis\n",
    "clf_Q1 = QuadraticDiscriminantAnalysis(reg_param=0.0001, store_covariance=True, tol=0.0001).fit(x_train, y_train)\n",
    "\n",
    "y_1 = clf_Q1.predict(x_test)\n",
    "score = roc_auc_score(y_test, y_1)\n",
    "#score = roc_auc_score(y_test, clf_1.predict_proba(x_test), multi_class='ovr')\n",
    "print('ROC AUC: %.3f' % score)\n",
    "print(classification_report_imbalanced(y_test, y_1))\n",
    "cm = metrics.confusion_matrix(y_test, y_1, normalize='true')\n",
    "cm_display = metrics.ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict SVC\n",
    "clf_S1 = SVC(C=0.1, gamma=1, kernel='rbf').fit(x_train, y_train)\n",
    "\n",
    "y_1 = clf_S1.predict(x_test)\n",
    "score = roc_auc_score(y_test, y_1)\n",
    "#score = roc_auc_score(y_test, clf_1.predict_proba(x_test), multi_class='ovr')\n",
    "print('ROC AUC: %.3f' % score)\n",
    "print(classification_report_imbalanced(y_test, y_1))\n",
    "cm = metrics.confusion_matrix(y_test, y_1, normalize='true')\n",
    "cm_display = metrics.ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict GradientBoostingClassifier\n",
    "clf_G1 = GradientBoostingClassifier(learning_rate=0.01, n_estimators=1000).fit(x_train, y_train)\n",
    "\n",
    "y_1 = clf_G1.predict(x_test)\n",
    "score = roc_auc_score(y_test, y_1)\n",
    "#score = roc_auc_score(y_test, clf_1.predict_proba(x_test), multi_class='ovr')\n",
    "print('ROC AUC: %.3f' % score)\n",
    "print(classification_report_imbalanced(y_test, y_1))\n",
    "cm = metrics.confusion_matrix(y_test, y_1, normalize='true')\n",
    "cm_display = metrics.ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict RandomForestClassifier\n",
    "clf_R1 = RandomForestClassifier(max_depth=6, n_estimators=300).fit(x_train, y_train)\n",
    "y_1 = clf_R1.predict(x_test)\n",
    "score = roc_auc_score(y_test, y_1)\n",
    "#score = roc_auc_score(y_test, clf_1.predict_proba(x_test), multi_class='ovr')\n",
    "print('ROC AUC: %.3f' % score)\n",
    "print(classification_report_imbalanced(y_test, y_1))\n",
    "cm = metrics.confusion_matrix(y_test, y_1, normalize='true')\n",
    "cm_display = metrics.ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.optimizers import adam_v2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n",
    "                              patience=8, min_lr=1e-6,verbose = 1)\n",
    "custom_early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', mode='min', verbose=1, patience=20\n",
    ")\n",
    "\n",
    "METRICS = [ \n",
    "      keras.metrics.AUC(name='auc'),\n",
    "]\n",
    "\n",
    "d1 = 0.15\n",
    "\n",
    "\n",
    "#model definition\n",
    "def create_model(learn_rate):\n",
    "    model = keras.Sequential([\n",
    "    keras.layers.Dense(100, activation=tf.nn.selu, kernel_initializer = initializers.he_normal,bias_initializer=initializers.he_normal, input_shape=(x_train.shape[1],)),\n",
    "        keras.layers.Dropout(d1),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        \n",
    "    keras.layers.Dense(80, activation=tf.nn.selu, kernel_constraint=max_norm(max_n), kernel_initializer = initializers.he_normal),\n",
    "        keras.layers.Dropout(d1),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        \n",
    "    keras.layers.Dense(70, activation=tf.nn.selu, kernel_constraint=max_norm(max_n), kernel_initializer = initializers.he_normal),\n",
    "        keras.layers.Dropout(d1),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        \n",
    "    keras.layers.Dense(50, activation=tf.nn.selu, kernel_constraint=max_norm(max_n), kernel_initializer = initializers.he_normal),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        \n",
    "    keras.layers.Dense(30, activation=tf.nn.selu, kernel_constraint=max_norm(max_n), kernel_initializer = initializers.he_normal),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        \n",
    "    keras.layers.Dense(10, activation=tf.nn.selu, kernel_constraint=max_norm(max_n), kernel_initializer = initializers.he_normal),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        \n",
    " keras.layers.Dense(1, activation=  'sigmoid')\n",
    " ])\n",
    "    optimizer = adam_v2.Adam(lr=learn_rate)\n",
    "    model.compile(loss='binary_crossentropy', metrics=[METRICS], optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Merge inputs and targets\n",
    "inputs = np.concatenate((x_train, x_test), axis=0)\n",
    "targets = np.concatenate((y_train, y_test), axis=0)\n",
    "\n",
    "max_n = 4.\n",
    "num_folds = 5\n",
    "\n",
    "# Define the K-fold Cross Validator\n",
    "kfold = StratifiedKFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "fold_no = 1\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "confusions_matrix = []\n",
    "for train, test in kfold.split(inputs, targets):\n",
    "      # Compile the model\n",
    "    model = create_model(0.1)\n",
    "    \n",
    "    X_train_res, y_train_res = inputs[train], targets[train]\n",
    "    X_test_res, y_test_res = inputs[test], targets[test]\n",
    "        \n",
    "      # Generate a print\n",
    "    print('-----------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "    # Fit data to model , class_weight=class_weight\n",
    "    history = model.fit(X_train_res, y_train_res, validation_data=(X_test_res, y_test_res),batch_size=256, epochs=100,callbacks=[custom_early_stopping, reduce_lr])\n",
    "\n",
    "    # Generate generalization metrics\n",
    "    scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
    "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "    acc_per_fold.append(scores[1] * 100)\n",
    "    loss_per_fold.append(scores[0])\n",
    "    \n",
    "    fig, axs = plt.subplots(2)\n",
    "    fig.suptitle('model loss')\n",
    "    fig.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "    axs[0].plot(history.history['loss'])\n",
    "    axs[0].plot(history.history['val_loss'])\n",
    "    axs[1].plot(history.history['auc'])\n",
    "    axs[1].plot(history.history['val_auc'])\n",
    "\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.show()\n",
    "\n",
    "      # Increase fold number\n",
    "    fold_no = fold_no + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict Neural network\n",
    "y_pred = model.predict(x_test)\n",
    "y_1 = np.round(y_pred)\n",
    "score = roc_auc_score(y_test, y_1)\n",
    "#score = roc_auc_score(y_test, clf_1.predict_proba(x_test), multi_class='ovr')\n",
    "print('ROC AUC: %.3f' % score)\n",
    "print(classification_report_imbalanced(y_test, y_1))\n",
    "cm = metrics.confusion_matrix(y_test, y_1, normalize='true')\n",
    "cm_display = metrics.ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feacture selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_data = data.copy()\n",
    "process_data = process_data[process_data.readmitted != 1]\n",
    "process_data = process_data.replace({'readmitted' : { 2 : 1}}) #output feacture\n",
    "y = pd.DataFrame(process_data.readmitted.copy())\n",
    "process_data = process_data.drop(columns=['readmitted'])\n",
    "x_train, x_test, y_train, y_test = train_test_split_generator(process_data, y, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "clf_R1 = RandomForestClassifier(max_depth=6, n_estimators=300,class_weight=class_weights)\n",
    "sfs = SequentialFeatureSelector(clf_R1, direction='backward', scoring='roc_auc_ovo')\n",
    "sfs.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_red = sfs.fit_transform(x_train)\n",
    "x_test_red = sfs.fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf_S1 = SVC(C=0.1, gamma=1, kernel='rbf').fit(x_train_red, y_train, sample_weight=train_sample_weight)\n",
    "clf_R1 = RandomForestClassifier(max_depth=6, n_estimators=300,class_weight=class_weights).fit(x_train_red, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict SVC\n",
    "y_1 = clf_S1.predict(x_test_red)\n",
    "score = roc_auc_score(y_test, y_1)\n",
    "#score = roc_auc_score(y_test, clf_1.predict_proba(x_test), multi_class='ovr')\n",
    "print('ROC AUC: %.3f' % score)\n",
    "print(classification_report_imbalanced(y_test, y_1))\n",
    "cm = metrics.confusion_matrix(y_test, y_1, normalize='true')\n",
    "cm_display = metrics.ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict RandomForestClassifier\n",
    "y_1 = clf_R1.predict(x_test_red)\n",
    "score = roc_auc_score(y_test, y_1)\n",
    "#score = roc_auc_score(y_test, clf_1.predict_proba(x_test), multi_class='ovr')\n",
    "print('ROC AUC: %.3f' % score)\n",
    "print(classification_report_imbalanced(y_test, y_1))\n",
    "cm = metrics.confusion_matrix(y_test, y_1, normalize='true')\n",
    "cm_display = metrics.ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.optimizers import adam_v2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n",
    "                              patience=8, min_lr=1e-6,verbose = 1)\n",
    "custom_early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', mode='min', verbose=1, patience=20\n",
    ")\n",
    "\n",
    "METRICS = [ \n",
    "      keras.metrics.AUC(name='auc'),\n",
    "]\n",
    "\n",
    "d1 = 0.15\n",
    "\n",
    "\n",
    "#model definition\n",
    "def create_model(learn_rate):\n",
    "    model = keras.Sequential([\n",
    "    keras.layers.Dense(100, activation=tf.nn.selu, kernel_initializer = initializers.he_normal,bias_initializer=initializers.he_normal, input_shape=(x_train.shape[1],)),\n",
    "        keras.layers.Dropout(d1),\n",
    "        keras.layers.BatchNormalization(),\n",
    "    \n",
    "    keras.layers.Dense(90, activation=tf.nn.selu, kernel_constraint=max_norm(max_n), kernel_initializer = initializers.he_normal),\n",
    "        keras.layers.Dropout(d1),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        \n",
    "    keras.layers.Dense(80, activation=tf.nn.selu, kernel_constraint=max_norm(max_n), kernel_initializer = initializers.he_normal),\n",
    "        keras.layers.Dropout(d1),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        \n",
    "    keras.layers.Dense(70, activation=tf.nn.selu, kernel_constraint=max_norm(max_n), kernel_initializer = initializers.he_normal),\n",
    "        keras.layers.Dropout(d1),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        \n",
    "    keras.layers.Dense(60, activation=tf.nn.selu, kernel_constraint=max_norm(max_n), kernel_initializer = initializers.he_normal),\n",
    "        keras.layers.Dropout(d1),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        \n",
    "    keras.layers.Dense(50, activation=tf.nn.selu, kernel_constraint=max_norm(max_n), kernel_initializer = initializers.he_normal),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        \n",
    "    keras.layers.Dense(40, activation=tf.nn.selu, kernel_constraint=max_norm(max_n), kernel_initializer = initializers.he_normal),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        \n",
    "    keras.layers.Dense(30, activation=tf.nn.selu, kernel_constraint=max_norm(max_n), kernel_initializer = initializers.he_normal),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        \n",
    "    keras.layers.Dense(20, activation=tf.nn.selu, kernel_constraint=max_norm(max_n), kernel_initializer = initializers.he_normal),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        \n",
    "    keras.layers.Dense(10, activation=tf.nn.selu, kernel_constraint=max_norm(max_n), kernel_initializer = initializers.he_normal),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        \n",
    " keras.layers.Dense(1, activation=  'sigmoid')\n",
    " ])\n",
    "    optimizer = adam_v2.Adam(lr=learn_rate)\n",
    "    model.compile(loss='binary_crossentropy', metrics=[METRICS], optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "x_train = x_train_red\n",
    "x_test = x_test_red\n",
    "\n",
    "# Merge inputs and targets\n",
    "inputs = np.concatenate((x_train, x_test), axis=0)\n",
    "targets = np.concatenate((y_train, y_test), axis=0)\n",
    "\n",
    "max_n = 4.\n",
    "num_folds = 5\n",
    "\n",
    "# Define the K-fold Cross Validator\n",
    "kfold = StratifiedKFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "fold_no = 1\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "confusions_matrix = []\n",
    "for train, test in kfold.split(inputs, targets):\n",
    "      # Compile the model\n",
    "    model = create_model(0.1)\n",
    "    \n",
    "    X_train_res, y_train_res = inputs[train], targets[train]\n",
    "    X_test_res, y_test_res = inputs[test], targets[test]\n",
    "        \n",
    "      # Generate a print\n",
    "    print('-----------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "    # Fit data to model , class_weight=class_weight\n",
    "    history = model.fit(X_train_res, y_train_res, class_weight=class_weights, validation_data=(X_test_res, y_test_res),batch_size=256, epochs=100,callbacks=[custom_early_stopping, reduce_lr])\n",
    "\n",
    "    # Generate generalization metrics\n",
    "    scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
    "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "    acc_per_fold.append(scores[1] * 100)\n",
    "    loss_per_fold.append(scores[0])\n",
    "    \n",
    "    fig, axs = plt.subplots(2)\n",
    "    fig.suptitle('model loss')\n",
    "    fig.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "    axs[0].plot(history.history['loss'])\n",
    "    axs[0].plot(history.history['val_loss'])\n",
    "    axs[1].plot(history.history['auc'])\n",
    "    axs[1].plot(history.history['val_auc'])\n",
    "\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.show()\n",
    "\n",
    "      # Increase fold number\n",
    "    fold_no = fold_no + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict Neural network\n",
    "y_pred = model.predict(x_test_red)\n",
    "y_1 = np.round(y_pred)\n",
    "score = roc_auc_score(y_test, y_1)\n",
    "#score = roc_auc_score(y_test, clf_1.predict_proba(x_test), multi_class='ovr')\n",
    "print('ROC AUC: %.3f' % score)\n",
    "print(classification_report_imbalanced(y_test, y_1))\n",
    "cm = metrics.confusion_matrix(y_test, y_1, normalize='true')\n",
    "cm_display = metrics.ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_data = data.copy()\n",
    "process_data = process_data[process_data.readmitted != 1]\n",
    "process_data = process_data.replace({'readmitted' : { 2 : 1}}) #output feacture\n",
    "y = pd.DataFrame(process_data.readmitted.copy())\n",
    "process_data = process_data.drop(columns=['readmitted'])\n",
    "x_train, x_test, y_train, y_test = train_test_split_generator(process_data, y, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca_test = PCA()\n",
    "pca_test.fit(x_train)\n",
    "sns.set(style='whitegrid')\n",
    "plt.plot(np.cumsum(pca_test.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance')\n",
    "plt.axvline(linewidth=4, color='r', linestyle = '--', x=20, ymin=0, ymax=1)\n",
    "display(plt.show())\n",
    "evr = pca_test.explained_variance_ratio_\n",
    "cvr = np.cumsum(pca_test.explained_variance_ratio_)\n",
    "pca_df = pd.DataFrame()\n",
    "pca_df['Cumulative Variance Ratio'] = cvr\n",
    "pca_df['Explained Variance Ratio'] = evr\n",
    "display(pca_df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=20)\n",
    "pca.fit(x_train)\n",
    "x_train_pca = pca.transform(x_train)\n",
    "x_test_pca = pca.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf_S1 = SVC(C=0.1, gamma=1, kernel='rbf').fit(x_train_pca, y_train, sample_weight=train_sample_weight)\n",
    "clf_R1 = RandomForestClassifier(max_depth=6, n_estimators=300,class_weight=class_weights).fit(x_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict SVC\n",
    "y_1 = clf_S1.predict(x_test_pca)\n",
    "score = roc_auc_score(y_test, y_1)\n",
    "#score = roc_auc_score(y_test, clf_1.predict_proba(x_test), multi_class='ovr')\n",
    "print('ROC AUC: %.3f' % score)\n",
    "print(classification_report_imbalanced(y_test, y_1))\n",
    "cm = metrics.confusion_matrix(y_test, y_1, normalize='true')\n",
    "cm_display = metrics.ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict RandomForestClassifier\n",
    "y_1 = clf_R1.predict(x_test_pca)\n",
    "score = roc_auc_score(y_test, y_1)\n",
    "#score = roc_auc_score(y_test, clf_1.predict_proba(x_test), multi_class='ovr')\n",
    "print('ROC AUC: %.3f' % score)\n",
    "print(classification_report_imbalanced(y_test, y_1))\n",
    "cm = metrics.confusion_matrix(y_test, y_1, normalize='true')\n",
    "cm_display = metrics.ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.optimizers import adam_v2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n",
    "                              patience=8, min_lr=1e-6,verbose = 1)\n",
    "custom_early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', mode='min', verbose=1, patience=20\n",
    ")\n",
    "\n",
    "METRICS = [ \n",
    "      keras.metrics.AUC(name='auc'),\n",
    "]\n",
    "\n",
    "d1 = 0.15\n",
    "\n",
    "\n",
    "#model definition\n",
    "def create_model(learn_rate):\n",
    "    model = keras.Sequential([\n",
    "    keras.layers.Dense(100, activation=tf.nn.selu, kernel_initializer = initializers.he_normal,bias_initializer=initializers.he_normal, input_shape=(x_train.shape[1],)),\n",
    "        keras.layers.Dropout(d1),\n",
    "        keras.layers.BatchNormalization(),\n",
    "    \n",
    "    keras.layers.Dense(90, activation=tf.nn.selu, kernel_constraint=max_norm(max_n), kernel_initializer = initializers.he_normal),\n",
    "        keras.layers.Dropout(d1),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        \n",
    "    keras.layers.Dense(80, activation=tf.nn.selu, kernel_constraint=max_norm(max_n), kernel_initializer = initializers.he_normal),\n",
    "        keras.layers.Dropout(d1),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        \n",
    "    keras.layers.Dense(70, activation=tf.nn.selu, kernel_constraint=max_norm(max_n), kernel_initializer = initializers.he_normal),\n",
    "        keras.layers.Dropout(d1),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        \n",
    "    keras.layers.Dense(60, activation=tf.nn.selu, kernel_constraint=max_norm(max_n), kernel_initializer = initializers.he_normal),\n",
    "        keras.layers.Dropout(d1),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        \n",
    "    keras.layers.Dense(50, activation=tf.nn.selu, kernel_constraint=max_norm(max_n), kernel_initializer = initializers.he_normal),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        \n",
    "    keras.layers.Dense(40, activation=tf.nn.selu, kernel_constraint=max_norm(max_n), kernel_initializer = initializers.he_normal),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        \n",
    "    keras.layers.Dense(30, activation=tf.nn.selu, kernel_constraint=max_norm(max_n), kernel_initializer = initializers.he_normal),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        \n",
    "    keras.layers.Dense(20, activation=tf.nn.selu, kernel_constraint=max_norm(max_n), kernel_initializer = initializers.he_normal),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        \n",
    "    keras.layers.Dense(10, activation=tf.nn.selu, kernel_constraint=max_norm(max_n), kernel_initializer = initializers.he_normal),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        \n",
    " keras.layers.Dense(1, activation=  'sigmoid')\n",
    " ])\n",
    "    optimizer = adam_v2.Adam(lr=learn_rate)\n",
    "    model.compile(loss='binary_crossentropy', metrics=[METRICS], optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "x_train = x_train_pca\n",
    "x_test = x_test_pca\n",
    "\n",
    "# Merge inputs and targets\n",
    "inputs = np.concatenate((x_train, x_test), axis=0)\n",
    "targets = np.concatenate((y_train, y_test), axis=0)\n",
    "\n",
    "max_n = 4.\n",
    "num_folds = 5\n",
    "\n",
    "# Define the K-fold Cross Validator\n",
    "kfold = StratifiedKFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "fold_no = 1\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "confusions_matrix = []\n",
    "for train, test in kfold.split(inputs, targets):\n",
    "      # Compile the model\n",
    "    model = create_model(0.1)\n",
    "    \n",
    "    X_train_res, y_train_res = inputs[train], targets[train]\n",
    "    X_test_res, y_test_res = inputs[test], targets[test]\n",
    "        \n",
    "      # Generate a print\n",
    "    print('-----------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "    # Fit data to model , class_weight=class_weight\n",
    "    history = model.fit(X_train_res, y_train_res, class_weight=class_weights, validation_data=(X_test_res, y_test_res),batch_size=256, epochs=100,callbacks=[custom_early_stopping, reduce_lr])\n",
    "\n",
    "    # Generate generalization metrics\n",
    "    scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
    "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "    acc_per_fold.append(scores[1] * 100)\n",
    "    loss_per_fold.append(scores[0])\n",
    "    \n",
    "    fig, axs = plt.subplots(2)\n",
    "    fig.suptitle('model loss')\n",
    "    fig.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "    axs[0].plot(history.history['loss'])\n",
    "    axs[0].plot(history.history['val_loss'])\n",
    "    axs[1].plot(history.history['auc'])\n",
    "    axs[1].plot(history.history['val_auc'])\n",
    "\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.show()\n",
    "\n",
    "      # Increase fold number\n",
    "    fold_no = fold_no + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict Neural network\n",
    "y_pred = model.predict(x_test_pca)\n",
    "y_1 = np.round(y_pred)\n",
    "score = roc_auc_score(y_test, y_1)\n",
    "#score = roc_auc_score(y_test, clf_1.predict_proba(x_test), multi_class='ovr')\n",
    "print('ROC AUC: %.3f' % score)\n",
    "print(classification_report_imbalanced(y_test, y_1))\n",
    "cm = metrics.confusion_matrix(y_test, y_1, normalize='true')\n",
    "cm_display = metrics.ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
